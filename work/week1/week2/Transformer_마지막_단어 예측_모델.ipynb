{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymxatB5WYxlL"
      },
      "source": [
        "# Transformer 실습\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 라이브러리 준비"
      ],
      "metadata": {
        "id": "QZQDhJginwTs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1X7RM2du1zcr",
        "outputId": "17f5e5e8-ff2d-4c30-bfe9-a88fb8c47bd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 데이터셋 준비"
      ],
      "metadata": {
        "id": "NLM4g-HUn2lf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HOdhoBVA1zcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c7ffa6-6729-4031-cb7a-457b3a40fc5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "# ds = load_dataset(\"stanfordnlp/imdb\")\n",
        "train_ds = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
        "test_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test\")\n",
        "\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "  max_len = 400\n",
        "  texts, labels = [], []\n",
        "\n",
        "  # 배치 데이터의 각 행마다 반복\n",
        "  for row in batch:\n",
        "    # truncation이 True -> max length가 넘어가면 자름. 끝에서 3번째 토큰을 label(정답)로 선택\n",
        "    labels.append(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[-3])\n",
        "    # 마지막 3개를 제외한 앞 부분의 토큰들을 텐서로 변환한 후 텍스트(입력)로 사용\n",
        "    texts.append(torch.LongTensor(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[:-3]))\n",
        "\n",
        "  # 패딩 토큰 추가 (아마 400자 기준이겠지?)\n",
        "  texts = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "  # labels를 텐서로 변환\n",
        "  labels = torch.LongTensor(labels)\n",
        "\n",
        "  return texts, labels\n",
        "\n",
        "# 각각 트레인 로더, 테스트 로더를 만들어준다. 배치사이즈는 64\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_ds, batch_size=64, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `tokenizer` → BERT 모델(bert-base-uncased)의 토크나이저를 가져옴\n",
        "    - bert : BERT 모델\n",
        "    - base : 12층 레이어, 은닉층 사이즈 768\n",
        "    - uncased  : 대소문자 구분 없이, 소문자로 바꿔서 처리함\n",
        "\n",
        "- 데이터의 배치 사이즈 64개 → 64개씩 묶어서 진행\n",
        "- label 값 = text 값(=리뷰 내용)의 토큰들 중 끝에서 3번째\n",
        "    - 왜 뒤에서 3번째일까…? 문장부호에 해당하는 토큰이나 `[SEP]` 토큰을 거르기 위해서인 것 같다.\n",
        "    - 아무래도 아래와 같이 토크나이즈 되는 경우가 많을 거라 생각된다.\n",
        "\n",
        "    ```['this', 'film', 'does', \"n't\", 'have', 'much', 'of', 'a', 'plot', '.', '[SEP]']```\n",
        "- 입력 값 = 처음부터 끝에서 3번째까지의 값"
      ],
      "metadata": {
        "id": "x5r3PxaUn6hP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### input 차원 체크"
      ],
      "metadata": {
        "id": "qiILm7PPoIhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text, label = next(iter(train_loader))\n",
        "print(text.shape, label.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dtsNOY6nzRc",
        "outputId": "85f84a79-1f78-46a3-99cc-92baed55255e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 397]) torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-FshZcTZBQ2"
      },
      "source": [
        "## Self-attention\n",
        "\n",
        "\n",
        "Self-attention은 shape이 (B, S, D)인 embedding이 들어왔을 때 attention을 적용하여 새로운 representation을 만들어내는 module입니다.\n",
        "여기서 B는 batch size, S는 sequence length, D는 embedding 차원입니다.\n",
        "구현은 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MBlMVMZcRAxv"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, input_dim, d_model):\n",
        "    super().__init__()\n",
        "\n",
        "    # 입력 벡터 차원\n",
        "    self.input_dim = input_dim\n",
        "\n",
        "    # Q, K, V 및 최종 출력 차원\n",
        "    self.d_model = d_model\n",
        "\n",
        "    # Q, K, V에 곱할 가중치 행렬값\n",
        "    self.wq = nn.Linear(input_dim, d_model)\n",
        "    self.wk = nn.Linear(input_dim, d_model)\n",
        "    self.wv = nn.Linear(input_dim, d_model)\n",
        "\n",
        "    # 은닉층 한 번 선형 결과를 만들어서 정확도를 더 높여준다\n",
        "    self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # 어텐션 스코어 계산에 필요한 소프트맥스. 확률 분포로 변환시켜준다. 0~1사이의 값\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    # Q, K, V 벡터 값 생성\n",
        "    q, k, v = self.wq(x), self.wk(x), self.wv(x) # (B, S, D)\n",
        "    # Q * K^T 쿼리와 키의 유사도를 구함\n",
        "    score = torch.matmul(q, k.transpose(-1, -2)) # (B, S, D) * (B, D, S) = (B, S, S)\n",
        "    # 스케일링 : 루트 d로 나누어서 크기 조정을 한다.\n",
        "    score = score / sqrt(self.d_model)\n",
        "\n",
        "    # 마스킹 - 패딩 토큰 무시. 0에 수렴하도록? 거의 안보이게 하기 위해 -1e9 더함\n",
        "    if mask is not None:\n",
        "      score = score + (mask * -1e9)\n",
        "\n",
        "    # 스코어를 소프트 맥스로 변환\n",
        "    score = self.softmax(score)\n",
        "    # 어텐션 스코어를 value의 벡터에 곱해서 문맥을 반영한 최종 벡터 꺼냄\n",
        "    result = torch.matmul(score, v) # -> (B, S, D)\n",
        "    # 최종 출력값 변환\n",
        "    result = self.dense(result) # -> (B, S, D)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 마스크에 대해 의문인 점 🙋‍♀️\n",
        "    - mask의 shape가 (B, S, 1)라고 써져있는데\n",
        "    - TextClassifier 클래스에서 가운데가 None인데 (B, S, 1) 아닌가..?\n",
        "    ```    \n",
        "    mask = (x == tokenizer.pad_token_id)\n",
        "    mask = mask[:, None, :]\n",
        "    ```"
      ],
      "metadata": {
        "id": "K58WgGnq27RS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S0vMp85ZRNO"
      },
      "source": [
        "대부분은 Transformer 챕터에서 배운 수식들을 그대로 구현한 것에 불과합니다.\n",
        "차이점은 `mask`의 존재여부입니다.\n",
        "이전 챕터에서 우리는 가변적인 text data들에 padding token을 붙여 하나의 matrix로 만든 방법을 배웠습니다.\n",
        "실제 attention 계산에서는 이를 무시해주기 위해 mask를 만들어 제공해주게 됩니다.\n",
        "여기서 mask의 shape은 (B, S, 1)로, 만약 `mask[i, j] = True`이면 그 변수는 padding token에 해당한다는 뜻입니다.\n",
        "이러한 값들을 무시해주는 방법은 shape이 (B, S, S)인 `score`가 있을 때(수업에서 배운 $A$와 동일) `score[i, j]`에 아주 작은 값을 더해주면 됩니다. 아주 작은 값은 예를 들어 `-1000..00 = -1e9` 같은 것이 있습니다.\n",
        "이렇게 작은 값을 더해주고 나면 softmax를 거쳤을 때 0에 가까워지기 때문에 weighted sum 과정에서 padding token에 해당하는 `v` 값들을 무시할 수 있게 됩니다.\n",
        "\n",
        "다음은 self-attention과 feed-forward layer를 구현한 모습입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 트랜스포머 레이어 정의"
      ],
      "metadata": {
        "id": "5dAPy8lK3HRQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VZHPCn9AS5Gp"
      },
      "outputs": [],
      "source": [
        "#트랜스포머\n",
        "class TransformerLayer(nn.Module):\n",
        "  def __init__(self, input_dim, d_model, dff):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim # 입력 벡터 차원\n",
        "    self.d_model = d_model # 모델 크기\n",
        "    self.dff = dff #은닉층 크기\n",
        "\n",
        "    self.sa = SelfAttention(input_dim, d_model) #어텐션 계산\n",
        "\n",
        "    # 그냥 우리가 알고있는 MLP\n",
        "    # 언어를 학습시키는 데 더 많은 가중치(weight)를 부여함\n",
        "    # 비선형 처리 + 추가 가중치\n",
        "    self.ffn = nn.Sequential(\n",
        "      nn.Linear(d_model, dff),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(dff, d_model)\n",
        "    )\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = self.sa(x, mask) #1. 셀프 어텐션의 결과를 가져온다.\n",
        "    x = self.ffn(x) #2. 어텐션 결과를 FFN(MLP)에 통과시킨다\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3VYrqTJagS1"
      },
      "source": [
        "## Positional encoding\n",
        "\n",
        "이번에는 positional encoding을 구현합니다. Positional encoding의 식은 다음과 같습니다:\n",
        "$$\n",
        "\\begin{align*} PE_{pos, 2i} &= \\sin\\left( \\frac{pos}{10000^{2i/D}} \\right), \\\\ PE_{pos, 2i+1} &= \\cos\\left( \\frac{pos}{10000^{2i/D}} \\right).\\end{align*}\n",
        "$$\n",
        "\n",
        "이를 Numpy로 구현하여 PyTorch tensor로 변환한 모습은 다음과 같습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf_jMQWDUR79",
        "outputId": "0a2f5d89-010c-4c39-ea01-1a5346e7de5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 400, 256])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "# 포지셔널 인코딩\n",
        "# 트랜스포머를 사용하려면 순서 정보 벡터값을 따로 계산해서 입력해줘야한다.\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[None, ...]\n",
        "\n",
        "    return torch.FloatTensor(pos_encoding)\n",
        "\n",
        "\n",
        "max_len = 400\n",
        "print(positional_encoding(max_len, 256).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5unoDcBva3eN"
      },
      "source": [
        "Positional encoding은 `angle_rads`를 구현하는 과정에서 모두 구현이 되었습니다. 여기서 `angle_rads`의 shape은 (S, D)입니다.\n",
        "우리는 일반적으로 batch로 주어지는 shape이 (B, S, D)인 tensor를 다루기 때문에 마지막에 None을 활용하여 shape을 (1, S, D)로 바꿔주게됩니다.\n",
        "\n",
        "위에서 구현한 `TransformerLayer`와 positional encoding을 모두 합친 모습은 다음과 같습니다\n",
        "\n",
        ".... 이 부분 어려워서 패스하겠습니다 😞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "8MaiCGh8TsDH"
      },
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_layers, dff):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.n_layers = n_layers\n",
        "    self.dff = dff\n",
        "\n",
        "    # 워드 임베딩\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    # 포지셔널 인코딩\n",
        "    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
        "    # rnn 대신 트랜스포머 레이어를 통과시킴.\n",
        "    # 인코더, 디코더를 여러개 두고 쓰는 방식이라서 여러개를 만들어서 통과시키는듯\n",
        "    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff) for _ in range(n_layers)])\n",
        "    # 마지막 토큰을 예측 -> 학습 데이터셋들의 모든 토큰들의 개수를 체크하는게 맞겠지 여기서 가장 유리한 것을 골라야하니까\n",
        "    self.classification = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x의 shape == (batch_size, seq_len, d_model)\n",
        "    # 마스크 정의: 패딩 토큰이 있다면 true\n",
        "    mask = (x == tokenizer.pad_token_id)\n",
        "    mask = mask[:, None, :] # -> (B,1,S)?\n",
        "\n",
        "    # 시퀀스 데이터의 길이\n",
        "    seq_len = x.shape[1]\n",
        "\n",
        "    # 워드 임베딩\n",
        "    x = self.embedding(x)\n",
        "    # 노멀라이즈?\n",
        "    x = x * sqrt(self.d_model)\n",
        "    # 포지셔널 인코딩 - 위치 정보 벡터 추가\n",
        "    x = x + self.pos_encoding[:, :seq_len]\n",
        "\n",
        "    # 트랜스포머의 레이어 갯수만큼 돌려라...\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "\n",
        "    x = x[:, 0] # 첫번째 차원(배치)에서 모든 요소를 선택, 두번째 차원(시퀀스 데이터) 첫번째 요소만 선택해서 마지막 단어를 예측하게끔 하는듯\n",
        "    x = self.classification(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "model = TextClassifier(len(tokenizer), 32, 2, 32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXpjPWHjbUK8"
      },
      "source": [
        "기존과 다른 점들은 다음과 같습니다:\n",
        "1. `nn.ModuleList`를 사용하여 여러 layer의 구현을 쉽게 하였습니다.\n",
        "2. Embedding, positional encoding, transformer layer를 거치고 난 후 마지막 label을 예측하기 위해 사용한 값은 `x[:, 0]`입니다. 기존의 RNN에서는 padding token을 제외한 마지막 token에 해당하는 representation을 사용한 것과 다릅니다. 이렇게 사용할 수 있는 이유는 attention 과정을 보시면 첫 번째 token에 대한 representation은 이후의 모든 token의 영향을 받습니다. 즉, 첫 번째 token 또한 전체 문장을 대변하는 의미를 가지고 있다고 할 수 있습니다. 그래서 일반적으로 Transformer를 text 분류에 사용할 때는 이와 같은 방식으로 구현됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDq05OlAb2lB"
      },
      "source": [
        "## 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YHVVsWBPQmnv"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "lr = 0.001\n",
        "model = model.to('cuda')\n",
        "# 교차 엔트로피로 수정\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "교차 엔트로피 손실 함수를 쓰는 이유?\n",
        "\n",
        "- 마지막 단어 예측 → 데이터셋의 모든 단어들 중 하나일 가능성일 확률이 높음\n",
        "- TextClassifier 클래스의 최종 출력 값의 shape vocab_size과 동일함\n",
        "    - 즉, MNIST 분류 문제처럼 모든 토큰들 중 하나를 분류하는 것.\n",
        "    - 교차 엔트로피는 분류 문제에서 예측값들의 정답 확률 분포를 알려줌\n",
        "    - `argmax(logits, dim=-1)` 을 통해 가장 높은 확률을 선택"
      ],
      "metadata": {
        "id": "rXRbQPSN3iol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "r88BALxO1zc1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def accuracy(model, dataloader):\n",
        "  cnt = 0\n",
        "  acc = 0\n",
        "\n",
        "  for data in dataloader:\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "    preds = model(inputs)\n",
        "    # 가장 큰 값이 곧 가장 정답에 가까운 예측값\n",
        "    preds = torch.argmax(preds, dim=-1)\n",
        "\n",
        "    cnt += labels.shape[0]\n",
        "    acc += (labels == preds).sum().item()\n",
        "\n",
        "  return acc / cnt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QnIUpvLaVaM",
        "outputId": "3794b869-6082-4586-97ab-23d73618b8e3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al_b56TYRILq",
        "outputId": "3347e11d-14d8-448c-e3ff-9d7b4ac942d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Train Loss: 3021.578178882599\n",
            "=========> Train acc: 0.039 | Test acc: 0.039\n",
            "Epoch   1 | Train Loss: 2695.5272641181946\n",
            "=========> Train acc: 0.039 | Test acc: 0.039\n",
            "Epoch   2 | Train Loss: 2666.333320617676\n",
            "=========> Train acc: 0.036 | Test acc: 0.034\n",
            "Epoch   3 | Train Loss: 2655.3203144073486\n",
            "=========> Train acc: 0.039 | Test acc: 0.039\n",
            "Epoch   4 | Train Loss: 2649.541160106659\n",
            "=========> Train acc: 0.042 | Test acc: 0.041\n",
            "Epoch   5 | Train Loss: 2644.4300560951233\n",
            "=========> Train acc: 0.041 | Test acc: 0.038\n",
            "Epoch   6 | Train Loss: 2638.7094736099243\n",
            "=========> Train acc: 0.041 | Test acc: 0.041\n",
            "Epoch   7 | Train Loss: 2624.6238555908203\n",
            "=========> Train acc: 0.040 | Test acc: 0.040\n",
            "Epoch   8 | Train Loss: 2607.0720953941345\n",
            "=========> Train acc: 0.042 | Test acc: 0.039\n",
            "Epoch   9 | Train Loss: 2591.545946121216\n",
            "=========> Train acc: 0.041 | Test acc: 0.037\n",
            "Epoch  10 | Train Loss: 2576.376081466675\n",
            "=========> Train acc: 0.044 | Test acc: 0.041\n",
            "Epoch  11 | Train Loss: 2564.9322957992554\n",
            "=========> Train acc: 0.046 | Test acc: 0.039\n",
            "Epoch  12 | Train Loss: 2552.908143043518\n",
            "=========> Train acc: 0.049 | Test acc: 0.042\n",
            "Epoch  13 | Train Loss: 2542.076898574829\n",
            "=========> Train acc: 0.052 | Test acc: 0.044\n",
            "Epoch  14 | Train Loss: 2530.6604285240173\n",
            "=========> Train acc: 0.053 | Test acc: 0.045\n",
            "Epoch  15 | Train Loss: 2516.6839628219604\n",
            "=========> Train acc: 0.056 | Test acc: 0.044\n",
            "Epoch  16 | Train Loss: 2503.7139229774475\n",
            "=========> Train acc: 0.057 | Test acc: 0.046\n",
            "Epoch  17 | Train Loss: 2490.228611469269\n",
            "=========> Train acc: 0.055 | Test acc: 0.042\n",
            "Epoch  18 | Train Loss: 2471.433894634247\n",
            "=========> Train acc: 0.059 | Test acc: 0.045\n",
            "Epoch  19 | Train Loss: 2455.64502286911\n",
            "=========> Train acc: 0.061 | Test acc: 0.045\n",
            "Epoch  20 | Train Loss: 2436.5443239212036\n",
            "=========> Train acc: 0.061 | Test acc: 0.042\n",
            "Epoch  21 | Train Loss: 2419.341715335846\n",
            "=========> Train acc: 0.060 | Test acc: 0.043\n",
            "Epoch  22 | Train Loss: 2408.9930276870728\n",
            "=========> Train acc: 0.064 | Test acc: 0.044\n",
            "Epoch  23 | Train Loss: 2394.5108399391174\n",
            "=========> Train acc: 0.067 | Test acc: 0.044\n",
            "Epoch  24 | Train Loss: 2374.0202798843384\n",
            "=========> Train acc: 0.071 | Test acc: 0.044\n",
            "Epoch  25 | Train Loss: 2361.4555897712708\n",
            "=========> Train acc: 0.070 | Test acc: 0.043\n",
            "Epoch  26 | Train Loss: 2344.044141769409\n",
            "=========> Train acc: 0.071 | Test acc: 0.044\n",
            "Epoch  27 | Train Loss: 2329.7444076538086\n",
            "=========> Train acc: 0.072 | Test acc: 0.045\n",
            "Epoch  28 | Train Loss: 2313.8951439857483\n",
            "=========> Train acc: 0.074 | Test acc: 0.042\n",
            "Epoch  29 | Train Loss: 2299.4343724250793\n",
            "=========> Train acc: 0.073 | Test acc: 0.042\n",
            "Epoch  30 | Train Loss: 2282.5908455848694\n",
            "=========> Train acc: 0.075 | Test acc: 0.041\n",
            "Epoch  31 | Train Loss: 2269.5894446372986\n",
            "=========> Train acc: 0.077 | Test acc: 0.042\n",
            "Epoch  32 | Train Loss: 2252.0222783088684\n",
            "=========> Train acc: 0.076 | Test acc: 0.040\n",
            "Epoch  33 | Train Loss: 2232.8331503868103\n",
            "=========> Train acc: 0.080 | Test acc: 0.042\n",
            "Epoch  34 | Train Loss: 2220.2238249778748\n",
            "=========> Train acc: 0.083 | Test acc: 0.043\n",
            "Epoch  35 | Train Loss: 2203.2780833244324\n",
            "=========> Train acc: 0.083 | Test acc: 0.041\n",
            "Epoch  36 | Train Loss: 2183.51087808609\n",
            "=========> Train acc: 0.082 | Test acc: 0.040\n",
            "Epoch  37 | Train Loss: 2170.1410913467407\n",
            "=========> Train acc: 0.084 | Test acc: 0.040\n",
            "Epoch  38 | Train Loss: 2152.742006778717\n",
            "=========> Train acc: 0.087 | Test acc: 0.039\n",
            "Epoch  39 | Train Loss: 2135.7672929763794\n",
            "=========> Train acc: 0.086 | Test acc: 0.043\n",
            "Epoch  40 | Train Loss: 2120.645248413086\n",
            "=========> Train acc: 0.090 | Test acc: 0.040\n",
            "Epoch  41 | Train Loss: 2112.3829946517944\n",
            "=========> Train acc: 0.087 | Test acc: 0.040\n",
            "Epoch  42 | Train Loss: 2098.974404811859\n",
            "=========> Train acc: 0.093 | Test acc: 0.038\n",
            "Epoch  43 | Train Loss: 2085.33935213089\n",
            "=========> Train acc: 0.095 | Test acc: 0.038\n",
            "Epoch  44 | Train Loss: 2065.955726623535\n",
            "=========> Train acc: 0.097 | Test acc: 0.038\n",
            "Epoch  45 | Train Loss: 2057.635359764099\n",
            "=========> Train acc: 0.102 | Test acc: 0.038\n",
            "Epoch  46 | Train Loss: 2041.6935992240906\n",
            "=========> Train acc: 0.100 | Test acc: 0.040\n",
            "Epoch  47 | Train Loss: 2034.548701763153\n",
            "=========> Train acc: 0.101 | Test acc: 0.039\n",
            "Epoch  48 | Train Loss: 2017.0547070503235\n",
            "=========> Train acc: 0.104 | Test acc: 0.037\n",
            "Epoch  49 | Train Loss: 2010.3251872062683\n",
            "=========> Train acc: 0.107 | Test acc: 0.036\n"
          ]
        }
      ],
      "source": [
        "\n",
        "n_epochs = 50\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  total_loss = 0.\n",
        "  model.train()\n",
        "  for data in train_loader:\n",
        "    model.zero_grad()\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "    # 수많은 값을 예측해서 가장 큰 값을 고르는게 포인트\n",
        "    preds = model(inputs)\n",
        "    loss = loss_fn(preds, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    train_acc = accuracy(model, train_loader)\n",
        "    test_acc = accuracy(model, test_loader)\n",
        "    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqZays2yb8Ja"
      },
      "source": [
        "정확도가 0.1 대인데.. 제대로 학습된 거 맞나요🥺"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import softmax\n",
        "\n",
        "# 1. 테스트 샘플 1개 가져오기\n",
        "sample = test_ds[18]['text']\n",
        "tokens = tokenizer(sample, truncation=True, max_length=400).input_ids\n",
        "\n",
        "# 2. 정답은 뒤에서 -3 인덱스인 토큰, 입력은 그 앞부분\n",
        "target_ids = tokens[-3]         # 정답\n",
        "input_ids = tokens[:-3]          # 입력\n",
        "\n",
        "# 3. 텐서로 변환해서 배치처럼 만들기\n",
        "input_tensor = torch.LongTensor([input_ids]).to(device)  # shape: (1, L)\n",
        "\n",
        "# 4. 모델에 넣기\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(input_tensor)\n",
        "    pred_ids = logits.argmax(dim=-1)[0]\n",
        "\n",
        "# 5. 예측 결과 디코딩\n",
        "pred_tokens = tokenizer.decode(pred_ids.tolist()) #토큰 ID -> 문자\n",
        "target_tokens = tokenizer.decode(target_ids)\n",
        "\n",
        "print(\"🟦 전체 입력 문장:\", sample)\n",
        "print(\"🟦 입력 문장:\", tokenizer.decode(input_ids))\n",
        "print(\"🟩 정답 토큰:\", target_tokens)\n",
        "print(\"🟥 예측 토큰:\", pred_tokens)\n"
      ],
      "metadata": {
        "id": "u-I121osywgW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83aeeaff-3008-4213-d1d3-d07ba4ed6b12"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🟦 전체 입력 문장: Ben, (Rupert Grint), is a deeply unhappy adolescent, the son of his unhappily married parents. His father, (Nicholas Farrell), is a vicar and his mother, (Laura Linney), is ... well, let's just say she's a somewhat hypocritical soldier in Jesus' army. It's only when he takes a summer job as an assistant to a foul-mouthed, eccentric, once-famous and now-forgotten actress Evie Walton, (Julie Walters), that he finally finds himself in true 'Harold and Maude' fashion. Of course, Evie is deeply unhappy herself and it's only when these two sad sacks find each other that they can put their mutual misery aside and hit the road to happiness.<br /><br />Of course it's corny and sentimental and very predictable but it has a hard side to it, too and Walters, who could sleep-walk her way through this sort of thing if she wanted, is excellent. It's when she puts the craziness to one side and finds the pathos in the character, (like hitting the bottle and throwing up in the sink), that she's at her best. The problem is she's the only interesting character in the film (and it's not because of the script which doesn't do anybody any favours). Grint, on the other hand, isn't just unhappy; he's a bit of a bore as well while Linney's starched bitch is completely one-dimensional. (Still, she's got the English accent off pat). The best that can be said for it is that it's mildly enjoyable - with the emphasis on the mildly.\n",
            "🟦 입력 문장: [CLS] ben, ( rupert grint ), is a deeply unhappy adolescent, the son of his unhappily married parents. his father, ( nicholas farrell ), is a vicar and his mother, ( laura linney ), is... well, let ' s just say she ' s a somewhat hypocritical soldier in jesus ' army. it ' s only when he takes a summer job as an assistant to a foul - mouthed, eccentric, once - famous and now - forgotten actress evie walton, ( julie walters ), that he finally finds himself in true ' harold and maude ' fashion. of course, evie is deeply unhappy herself and it ' s only when these two sad sacks find each other that they can put their mutual misery aside and hit the road to happiness. < br / > < br / > of course it ' s corny and sentimental and very predictable but it has a hard side to it, too and walters, who could sleep - walk her way through this sort of thing if she wanted, is excellent. it ' s when she puts the craziness to one side and finds the pathos in the character, ( like hitting the bottle and throwing up in the sink ), that she ' s at her best. the problem is she ' s the only interesting character in the film ( and it ' s not because of the script which doesn ' t do anybody any favours ). grint, on the other hand, isn ' t just unhappy ; he ' s a bit of a bore as well while linney ' s starched bitch is completely one - dimensional. ( still, she ' s got the english accent off pat ). the best that can be said for it is that it ' s mildly enjoyable - with the emphasis on the\n",
            "🟩 정답 토큰: mildly\n",
            "🟥 예측 토큰: .\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "name": "Transformer_ipynb의_사본.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}