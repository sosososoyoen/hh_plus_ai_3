{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymxatB5WYxlL"
      },
      "source": [
        "# Transformer ì‹¤ìŠµ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤€ë¹„"
      ],
      "metadata": {
        "id": "QZQDhJginwTs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1X7RM2du1zcr",
        "outputId": "17f5e5e8-ff2d-4c30-bfe9-a88fb8c47bd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. ë°ì´í„°ì…‹ ì¤€ë¹„"
      ],
      "metadata": {
        "id": "NLM4g-HUn2lf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HOdhoBVA1zcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c7ffa6-6729-4031-cb7a-457b3a40fc5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "# ds = load_dataset(\"stanfordnlp/imdb\")\n",
        "train_ds = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
        "test_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test\")\n",
        "\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "  max_len = 400\n",
        "  texts, labels = [], []\n",
        "\n",
        "  # ë°°ì¹˜ ë°ì´í„°ì˜ ê° í–‰ë§ˆë‹¤ ë°˜ë³µ\n",
        "  for row in batch:\n",
        "    # truncationì´ True -> max lengthê°€ ë„˜ì–´ê°€ë©´ ìë¦„. ëì—ì„œ 3ë²ˆì§¸ í† í°ì„ label(ì •ë‹µ)ë¡œ ì„ íƒ\n",
        "    labels.append(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[-3])\n",
        "    # ë§ˆì§€ë§‰ 3ê°œë¥¼ ì œì™¸í•œ ì• ë¶€ë¶„ì˜ í† í°ë“¤ì„ í…ì„œë¡œ ë³€í™˜í•œ í›„ í…ìŠ¤íŠ¸(ì…ë ¥)ë¡œ ì‚¬ìš©\n",
        "    texts.append(torch.LongTensor(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[:-3]))\n",
        "\n",
        "  # íŒ¨ë”© í† í° ì¶”ê°€ (ì•„ë§ˆ 400ì ê¸°ì¤€ì´ê² ì§€?)\n",
        "  texts = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "  # labelsë¥¼ í…ì„œë¡œ ë³€í™˜\n",
        "  labels = torch.LongTensor(labels)\n",
        "\n",
        "  return texts, labels\n",
        "\n",
        "# ê°ê° íŠ¸ë ˆì¸ ë¡œë”, í…ŒìŠ¤íŠ¸ ë¡œë”ë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤. ë°°ì¹˜ì‚¬ì´ì¦ˆëŠ” 64\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_ds, batch_size=64, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `tokenizer` â†’ BERT ëª¨ë¸(bert-base-uncased)ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ê°€ì ¸ì˜´\n",
        "    - bert : BERT ëª¨ë¸\n",
        "    - base : 12ì¸µ ë ˆì´ì–´, ì€ë‹‰ì¸µ ì‚¬ì´ì¦ˆ 768\n",
        "    - uncased  : ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´, ì†Œë¬¸ìë¡œ ë°”ê¿”ì„œ ì²˜ë¦¬í•¨\n",
        "\n",
        "- ë°ì´í„°ì˜ ë°°ì¹˜ ì‚¬ì´ì¦ˆ 64ê°œ â†’ 64ê°œì”© ë¬¶ì–´ì„œ ì§„í–‰\n",
        "- label ê°’ = text ê°’(=ë¦¬ë·° ë‚´ìš©)ì˜ í† í°ë“¤ ì¤‘ ëì—ì„œ 3ë²ˆì§¸\n",
        "    - ì™œ ë’¤ì—ì„œ 3ë²ˆì§¸ì¼ê¹Œâ€¦? ë¬¸ì¥ë¶€í˜¸ì— í•´ë‹¹í•˜ëŠ” í† í°ì´ë‚˜ `[SEP]` í† í°ì„ ê±°ë¥´ê¸° ìœ„í•´ì„œì¸ ê²ƒ ê°™ë‹¤.\n",
        "    - ì•„ë¬´ë˜ë„ ì•„ë˜ì™€ ê°™ì´ í† í¬ë‚˜ì´ì¦ˆ ë˜ëŠ” ê²½ìš°ê°€ ë§ì„ ê±°ë¼ ìƒê°ëœë‹¤.\n",
        "\n",
        "    ```['this', 'film', 'does', \"n't\", 'have', 'much', 'of', 'a', 'plot', '.', '[SEP]']```\n",
        "- ì…ë ¥ ê°’ = ì²˜ìŒë¶€í„° ëì—ì„œ 3ë²ˆì§¸ê¹Œì§€ì˜ ê°’"
      ],
      "metadata": {
        "id": "x5r3PxaUn6hP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### input ì°¨ì› ì²´í¬"
      ],
      "metadata": {
        "id": "qiILm7PPoIhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text, label = next(iter(train_loader))\n",
        "print(text.shape, label.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dtsNOY6nzRc",
        "outputId": "85f84a79-1f78-46a3-99cc-92baed55255e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 397]) torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-FshZcTZBQ2"
      },
      "source": [
        "## Self-attention\n",
        "\n",
        "\n",
        "Self-attentionì€ shapeì´ (B, S, D)ì¸ embeddingì´ ë“¤ì–´ì™”ì„ ë•Œ attentionì„ ì ìš©í•˜ì—¬ ìƒˆë¡œìš´ representationì„ ë§Œë“¤ì–´ë‚´ëŠ” moduleì…ë‹ˆë‹¤.\n",
        "ì—¬ê¸°ì„œ BëŠ” batch size, SëŠ” sequence length, DëŠ” embedding ì°¨ì›ì…ë‹ˆë‹¤.\n",
        "êµ¬í˜„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MBlMVMZcRAxv"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, input_dim, d_model):\n",
        "    super().__init__()\n",
        "\n",
        "    # ì…ë ¥ ë²¡í„° ì°¨ì›\n",
        "    self.input_dim = input_dim\n",
        "\n",
        "    # Q, K, V ë° ìµœì¢… ì¶œë ¥ ì°¨ì›\n",
        "    self.d_model = d_model\n",
        "\n",
        "    # Q, K, Vì— ê³±í•  ê°€ì¤‘ì¹˜ í–‰ë ¬ê°’\n",
        "    self.wq = nn.Linear(input_dim, d_model)\n",
        "    self.wk = nn.Linear(input_dim, d_model)\n",
        "    self.wv = nn.Linear(input_dim, d_model)\n",
        "\n",
        "    # ì€ë‹‰ì¸µ í•œ ë²ˆ ì„ í˜• ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ì„œ ì •í™•ë„ë¥¼ ë” ë†’ì—¬ì¤€ë‹¤\n",
        "    self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°ì— í•„ìš”í•œ ì†Œí”„íŠ¸ë§¥ìŠ¤. í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜ì‹œì¼œì¤€ë‹¤. 0~1ì‚¬ì´ì˜ ê°’\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    # Q, K, V ë²¡í„° ê°’ ìƒì„±\n",
        "    q, k, v = self.wq(x), self.wk(x), self.wv(x) # (B, S, D)\n",
        "    # Q * K^T ì¿¼ë¦¬ì™€ í‚¤ì˜ ìœ ì‚¬ë„ë¥¼ êµ¬í•¨\n",
        "    score = torch.matmul(q, k.transpose(-1, -2)) # (B, S, D) * (B, D, S) = (B, S, S)\n",
        "    # ìŠ¤ì¼€ì¼ë§ : ë£¨íŠ¸ dë¡œ ë‚˜ëˆ„ì–´ì„œ í¬ê¸° ì¡°ì •ì„ í•œë‹¤.\n",
        "    score = score / sqrt(self.d_model)\n",
        "\n",
        "    # ë§ˆìŠ¤í‚¹ - íŒ¨ë”© í† í° ë¬´ì‹œ. 0ì— ìˆ˜ë ´í•˜ë„ë¡? ê±°ì˜ ì•ˆë³´ì´ê²Œ í•˜ê¸° ìœ„í•´ -1e9 ë”í•¨\n",
        "    if mask is not None:\n",
        "      score = score + (mask * -1e9)\n",
        "\n",
        "    # ìŠ¤ì½”ì–´ë¥¼ ì†Œí”„íŠ¸ ë§¥ìŠ¤ë¡œ ë³€í™˜\n",
        "    score = self.softmax(score)\n",
        "    # ì–´í…ì…˜ ìŠ¤ì½”ì–´ë¥¼ valueì˜ ë²¡í„°ì— ê³±í•´ì„œ ë¬¸ë§¥ì„ ë°˜ì˜í•œ ìµœì¢… ë²¡í„° êº¼ëƒ„\n",
        "    result = torch.matmul(score, v) # -> (B, S, D)\n",
        "    # ìµœì¢… ì¶œë ¥ê°’ ë³€í™˜\n",
        "    result = self.dense(result) # -> (B, S, D)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ë§ˆìŠ¤í¬ì— ëŒ€í•´ ì˜ë¬¸ì¸ ì  ğŸ™‹â€â™€ï¸\n",
        "    - maskì˜ shapeê°€ (B, S, 1)ë¼ê³  ì¨ì ¸ìˆëŠ”ë°\n",
        "    - TextClassifier í´ë˜ìŠ¤ì—ì„œ ê°€ìš´ë°ê°€ Noneì¸ë° (B, S, 1) ì•„ë‹Œê°€..?\n",
        "    ```    \n",
        "    mask = (x == tokenizer.pad_token_id)\n",
        "    mask = mask[:, None, :]\n",
        "    ```"
      ],
      "metadata": {
        "id": "K58WgGnq27RS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S0vMp85ZRNO"
      },
      "source": [
        "ëŒ€ë¶€ë¶„ì€ Transformer ì±•í„°ì—ì„œ ë°°ìš´ ìˆ˜ì‹ë“¤ì„ ê·¸ëŒ€ë¡œ êµ¬í˜„í•œ ê²ƒì— ë¶ˆê³¼í•©ë‹ˆë‹¤.\n",
        "ì°¨ì´ì ì€ `mask`ì˜ ì¡´ì¬ì—¬ë¶€ì…ë‹ˆë‹¤.\n",
        "ì´ì „ ì±•í„°ì—ì„œ ìš°ë¦¬ëŠ” ê°€ë³€ì ì¸ text dataë“¤ì— padding tokenì„ ë¶™ì—¬ í•˜ë‚˜ì˜ matrixë¡œ ë§Œë“  ë°©ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤.\n",
        "ì‹¤ì œ attention ê³„ì‚°ì—ì„œëŠ” ì´ë¥¼ ë¬´ì‹œí•´ì£¼ê¸° ìœ„í•´ maskë¥¼ ë§Œë“¤ì–´ ì œê³µí•´ì£¼ê²Œ ë©ë‹ˆë‹¤.\n",
        "ì—¬ê¸°ì„œ maskì˜ shapeì€ (B, S, 1)ë¡œ, ë§Œì•½ `mask[i, j] = True`ì´ë©´ ê·¸ ë³€ìˆ˜ëŠ” padding tokenì— í•´ë‹¹í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.\n",
        "ì´ëŸ¬í•œ ê°’ë“¤ì„ ë¬´ì‹œí•´ì£¼ëŠ” ë°©ë²•ì€ shapeì´ (B, S, S)ì¸ `score`ê°€ ìˆì„ ë•Œ(ìˆ˜ì—…ì—ì„œ ë°°ìš´ $A$ì™€ ë™ì¼) `score[i, j]`ì— ì•„ì£¼ ì‘ì€ ê°’ì„ ë”í•´ì£¼ë©´ ë©ë‹ˆë‹¤. ì•„ì£¼ ì‘ì€ ê°’ì€ ì˜ˆë¥¼ ë“¤ì–´ `-1000..00 = -1e9` ê°™ì€ ê²ƒì´ ìˆìŠµë‹ˆë‹¤.\n",
        "ì´ë ‡ê²Œ ì‘ì€ ê°’ì„ ë”í•´ì£¼ê³  ë‚˜ë©´ softmaxë¥¼ ê±°ì³¤ì„ ë•Œ 0ì— ê°€ê¹Œì›Œì§€ê¸° ë•Œë¬¸ì— weighted sum ê³¼ì •ì—ì„œ padding tokenì— í•´ë‹¹í•˜ëŠ” `v` ê°’ë“¤ì„ ë¬´ì‹œí•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ë‹¤ìŒì€ self-attentionê³¼ feed-forward layerë¥¼ êµ¬í˜„í•œ ëª¨ìŠµì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ì •ì˜"
      ],
      "metadata": {
        "id": "5dAPy8lK3HRQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VZHPCn9AS5Gp"
      },
      "outputs": [],
      "source": [
        "#íŠ¸ëœìŠ¤í¬ë¨¸\n",
        "class TransformerLayer(nn.Module):\n",
        "  def __init__(self, input_dim, d_model, dff):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim # ì…ë ¥ ë²¡í„° ì°¨ì›\n",
        "    self.d_model = d_model # ëª¨ë¸ í¬ê¸°\n",
        "    self.dff = dff #ì€ë‹‰ì¸µ í¬ê¸°\n",
        "\n",
        "    self.sa = SelfAttention(input_dim, d_model) #ì–´í…ì…˜ ê³„ì‚°\n",
        "\n",
        "    # ê·¸ëƒ¥ ìš°ë¦¬ê°€ ì•Œê³ ìˆëŠ” MLP\n",
        "    # ì–¸ì–´ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ë° ë” ë§ì€ ê°€ì¤‘ì¹˜(weight)ë¥¼ ë¶€ì—¬í•¨\n",
        "    # ë¹„ì„ í˜• ì²˜ë¦¬ + ì¶”ê°€ ê°€ì¤‘ì¹˜\n",
        "    self.ffn = nn.Sequential(\n",
        "      nn.Linear(d_model, dff),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(dff, d_model)\n",
        "    )\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = self.sa(x, mask) #1. ì…€í”„ ì–´í…ì…˜ì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¨ë‹¤.\n",
        "    x = self.ffn(x) #2. ì–´í…ì…˜ ê²°ê³¼ë¥¼ FFN(MLP)ì— í†µê³¼ì‹œí‚¨ë‹¤\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3VYrqTJagS1"
      },
      "source": [
        "## Positional encoding\n",
        "\n",
        "ì´ë²ˆì—ëŠ” positional encodingì„ êµ¬í˜„í•©ë‹ˆë‹¤. Positional encodingì˜ ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
        "$$\n",
        "\\begin{align*} PE_{pos, 2i} &= \\sin\\left( \\frac{pos}{10000^{2i/D}} \\right), \\\\ PE_{pos, 2i+1} &= \\cos\\left( \\frac{pos}{10000^{2i/D}} \\right).\\end{align*}\n",
        "$$\n",
        "\n",
        "ì´ë¥¼ Numpyë¡œ êµ¬í˜„í•˜ì—¬ PyTorch tensorë¡œ ë³€í™˜í•œ ëª¨ìŠµì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf_jMQWDUR79",
        "outputId": "0a2f5d89-010c-4c39-ea01-1a5346e7de5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 400, 256])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "# í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
        "# íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ìˆœì„œ ì •ë³´ ë²¡í„°ê°’ì„ ë”°ë¡œ ê³„ì‚°í•´ì„œ ì…ë ¥í•´ì¤˜ì•¼í•œë‹¤.\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[None, ...]\n",
        "\n",
        "    return torch.FloatTensor(pos_encoding)\n",
        "\n",
        "\n",
        "max_len = 400\n",
        "print(positional_encoding(max_len, 256).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5unoDcBva3eN"
      },
      "source": [
        "Positional encodingì€ `angle_rads`ë¥¼ êµ¬í˜„í•˜ëŠ” ê³¼ì •ì—ì„œ ëª¨ë‘ êµ¬í˜„ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ `angle_rads`ì˜ shapeì€ (S, D)ì…ë‹ˆë‹¤.\n",
        "ìš°ë¦¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ batchë¡œ ì£¼ì–´ì§€ëŠ” shapeì´ (B, S, D)ì¸ tensorë¥¼ ë‹¤ë£¨ê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ì— Noneì„ í™œìš©í•˜ì—¬ shapeì„ (1, S, D)ë¡œ ë°”ê¿”ì£¼ê²Œë©ë‹ˆë‹¤.\n",
        "\n",
        "ìœ„ì—ì„œ êµ¬í˜„í•œ `TransformerLayer`ì™€ positional encodingì„ ëª¨ë‘ í•©ì¹œ ëª¨ìŠµì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤\n",
        "\n",
        ".... ì´ ë¶€ë¶„ ì–´ë ¤ì›Œì„œ íŒ¨ìŠ¤í•˜ê² ìŠµë‹ˆë‹¤ ğŸ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "8MaiCGh8TsDH"
      },
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_layers, dff):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.n_layers = n_layers\n",
        "    self.dff = dff\n",
        "\n",
        "    # ì›Œë“œ ì„ë² ë”©\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    # í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
        "    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
        "    # rnn ëŒ€ì‹  íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë¥¼ í†µê³¼ì‹œí‚´.\n",
        "    # ì¸ì½”ë”, ë””ì½”ë”ë¥¼ ì—¬ëŸ¬ê°œ ë‘ê³  ì“°ëŠ” ë°©ì‹ì´ë¼ì„œ ì—¬ëŸ¬ê°œë¥¼ ë§Œë“¤ì–´ì„œ í†µê³¼ì‹œí‚¤ëŠ”ë“¯\n",
        "    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff) for _ in range(n_layers)])\n",
        "    # ë§ˆì§€ë§‰ í† í°ì„ ì˜ˆì¸¡ -> í•™ìŠµ ë°ì´í„°ì…‹ë“¤ì˜ ëª¨ë“  í† í°ë“¤ì˜ ê°œìˆ˜ë¥¼ ì²´í¬í•˜ëŠ”ê²Œ ë§ê² ì§€ ì—¬ê¸°ì„œ ê°€ì¥ ìœ ë¦¬í•œ ê²ƒì„ ê³¨ë¼ì•¼í•˜ë‹ˆê¹Œ\n",
        "    self.classification = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # xì˜ shape == (batch_size, seq_len, d_model)\n",
        "    # ë§ˆìŠ¤í¬ ì •ì˜: íŒ¨ë”© í† í°ì´ ìˆë‹¤ë©´ true\n",
        "    mask = (x == tokenizer.pad_token_id)\n",
        "    mask = mask[:, None, :] # -> (B,1,S)?\n",
        "\n",
        "    # ì‹œí€€ìŠ¤ ë°ì´í„°ì˜ ê¸¸ì´\n",
        "    seq_len = x.shape[1]\n",
        "\n",
        "    # ì›Œë“œ ì„ë² ë”©\n",
        "    x = self.embedding(x)\n",
        "    # ë…¸ë©€ë¼ì´ì¦ˆ?\n",
        "    x = x * sqrt(self.d_model)\n",
        "    # í¬ì§€ì…”ë„ ì¸ì½”ë”© - ìœ„ì¹˜ ì •ë³´ ë²¡í„° ì¶”ê°€\n",
        "    x = x + self.pos_encoding[:, :seq_len]\n",
        "\n",
        "    # íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë ˆì´ì–´ ê°¯ìˆ˜ë§Œí¼ ëŒë ¤ë¼...\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "\n",
        "    x = x[:, 0] # ì²«ë²ˆì§¸ ì°¨ì›(ë°°ì¹˜)ì—ì„œ ëª¨ë“  ìš”ì†Œë¥¼ ì„ íƒ, ë‘ë²ˆì§¸ ì°¨ì›(ì‹œí€€ìŠ¤ ë°ì´í„°) ì²«ë²ˆì§¸ ìš”ì†Œë§Œ ì„ íƒí•´ì„œ ë§ˆì§€ë§‰ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê²Œë” í•˜ëŠ”ë“¯\n",
        "    x = self.classification(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "model = TextClassifier(len(tokenizer), 32, 2, 32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXpjPWHjbUK8"
      },
      "source": [
        "ê¸°ì¡´ê³¼ ë‹¤ë¥¸ ì ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
        "1. `nn.ModuleList`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ layerì˜ êµ¬í˜„ì„ ì‰½ê²Œ í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
        "2. Embedding, positional encoding, transformer layerë¥¼ ê±°ì¹˜ê³  ë‚œ í›„ ë§ˆì§€ë§‰ labelì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•œ ê°’ì€ `x[:, 0]`ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ RNNì—ì„œëŠ” padding tokenì„ ì œì™¸í•œ ë§ˆì§€ë§‰ tokenì— í•´ë‹¹í•˜ëŠ” representationì„ ì‚¬ìš©í•œ ê²ƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ì´ë ‡ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì´ìœ ëŠ” attention ê³¼ì •ì„ ë³´ì‹œë©´ ì²« ë²ˆì§¸ tokenì— ëŒ€í•œ representationì€ ì´í›„ì˜ ëª¨ë“  tokenì˜ ì˜í–¥ì„ ë°›ìŠµë‹ˆë‹¤. ì¦‰, ì²« ë²ˆì§¸ token ë˜í•œ ì „ì²´ ë¬¸ì¥ì„ ëŒ€ë³€í•˜ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì¼ë°˜ì ìœ¼ë¡œ Transformerë¥¼ text ë¶„ë¥˜ì— ì‚¬ìš©í•  ë•ŒëŠ” ì´ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDq05OlAb2lB"
      },
      "source": [
        "## í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YHVVsWBPQmnv"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "lr = 0.001\n",
        "model = model.to('cuda')\n",
        "# êµì°¨ ì—”íŠ¸ë¡œí”¼ë¡œ ìˆ˜ì •\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì“°ëŠ” ì´ìœ ?\n",
        "\n",
        "- ë§ˆì§€ë§‰ ë‹¨ì–´ ì˜ˆì¸¡ â†’ ë°ì´í„°ì…‹ì˜ ëª¨ë“  ë‹¨ì–´ë“¤ ì¤‘ í•˜ë‚˜ì¼ ê°€ëŠ¥ì„±ì¼ í™•ë¥ ì´ ë†’ìŒ\n",
        "- TextClassifier í´ë˜ìŠ¤ì˜ ìµœì¢… ì¶œë ¥ ê°’ì˜ shape vocab_sizeê³¼ ë™ì¼í•¨\n",
        "    - ì¦‰, MNIST ë¶„ë¥˜ ë¬¸ì œì²˜ëŸ¼ ëª¨ë“  í† í°ë“¤ ì¤‘ í•˜ë‚˜ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê²ƒ.\n",
        "    - êµì°¨ ì—”íŠ¸ë¡œí”¼ëŠ” ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì˜ˆì¸¡ê°’ë“¤ì˜ ì •ë‹µ í™•ë¥  ë¶„í¬ë¥¼ ì•Œë ¤ì¤Œ\n",
        "    - `argmax(logits, dim=-1)` ì„ í†µí•´ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ì„ íƒ"
      ],
      "metadata": {
        "id": "rXRbQPSN3iol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "r88BALxO1zc1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def accuracy(model, dataloader):\n",
        "  cnt = 0\n",
        "  acc = 0\n",
        "\n",
        "  for data in dataloader:\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "    preds = model(inputs)\n",
        "    # ê°€ì¥ í° ê°’ì´ ê³§ ê°€ì¥ ì •ë‹µì— ê°€ê¹Œìš´ ì˜ˆì¸¡ê°’\n",
        "    preds = torch.argmax(preds, dim=-1)\n",
        "\n",
        "    cnt += labels.shape[0]\n",
        "    acc += (labels == preds).sum().item()\n",
        "\n",
        "  return acc / cnt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QnIUpvLaVaM",
        "outputId": "3794b869-6082-4586-97ab-23d73618b8e3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al_b56TYRILq",
        "outputId": "3347e11d-14d8-448c-e3ff-9d7b4ac942d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Train Loss: 3021.578178882599\n",
            "=========> Train acc: 0.039 | Test acc: 0.039\n",
            "Epoch   1 | Train Loss: 2695.5272641181946\n",
            "=========> Train acc: 0.039 | Test acc: 0.039\n",
            "Epoch   2 | Train Loss: 2666.333320617676\n",
            "=========> Train acc: 0.036 | Test acc: 0.034\n",
            "Epoch   3 | Train Loss: 2655.3203144073486\n",
            "=========> Train acc: 0.039 | Test acc: 0.039\n",
            "Epoch   4 | Train Loss: 2649.541160106659\n",
            "=========> Train acc: 0.042 | Test acc: 0.041\n",
            "Epoch   5 | Train Loss: 2644.4300560951233\n",
            "=========> Train acc: 0.041 | Test acc: 0.038\n",
            "Epoch   6 | Train Loss: 2638.7094736099243\n",
            "=========> Train acc: 0.041 | Test acc: 0.041\n",
            "Epoch   7 | Train Loss: 2624.6238555908203\n",
            "=========> Train acc: 0.040 | Test acc: 0.040\n",
            "Epoch   8 | Train Loss: 2607.0720953941345\n",
            "=========> Train acc: 0.042 | Test acc: 0.039\n",
            "Epoch   9 | Train Loss: 2591.545946121216\n",
            "=========> Train acc: 0.041 | Test acc: 0.037\n",
            "Epoch  10 | Train Loss: 2576.376081466675\n",
            "=========> Train acc: 0.044 | Test acc: 0.041\n",
            "Epoch  11 | Train Loss: 2564.9322957992554\n",
            "=========> Train acc: 0.046 | Test acc: 0.039\n",
            "Epoch  12 | Train Loss: 2552.908143043518\n",
            "=========> Train acc: 0.049 | Test acc: 0.042\n",
            "Epoch  13 | Train Loss: 2542.076898574829\n",
            "=========> Train acc: 0.052 | Test acc: 0.044\n",
            "Epoch  14 | Train Loss: 2530.6604285240173\n",
            "=========> Train acc: 0.053 | Test acc: 0.045\n",
            "Epoch  15 | Train Loss: 2516.6839628219604\n",
            "=========> Train acc: 0.056 | Test acc: 0.044\n",
            "Epoch  16 | Train Loss: 2503.7139229774475\n",
            "=========> Train acc: 0.057 | Test acc: 0.046\n",
            "Epoch  17 | Train Loss: 2490.228611469269\n",
            "=========> Train acc: 0.055 | Test acc: 0.042\n",
            "Epoch  18 | Train Loss: 2471.433894634247\n",
            "=========> Train acc: 0.059 | Test acc: 0.045\n",
            "Epoch  19 | Train Loss: 2455.64502286911\n",
            "=========> Train acc: 0.061 | Test acc: 0.045\n",
            "Epoch  20 | Train Loss: 2436.5443239212036\n",
            "=========> Train acc: 0.061 | Test acc: 0.042\n",
            "Epoch  21 | Train Loss: 2419.341715335846\n",
            "=========> Train acc: 0.060 | Test acc: 0.043\n",
            "Epoch  22 | Train Loss: 2408.9930276870728\n",
            "=========> Train acc: 0.064 | Test acc: 0.044\n",
            "Epoch  23 | Train Loss: 2394.5108399391174\n",
            "=========> Train acc: 0.067 | Test acc: 0.044\n",
            "Epoch  24 | Train Loss: 2374.0202798843384\n",
            "=========> Train acc: 0.071 | Test acc: 0.044\n",
            "Epoch  25 | Train Loss: 2361.4555897712708\n",
            "=========> Train acc: 0.070 | Test acc: 0.043\n",
            "Epoch  26 | Train Loss: 2344.044141769409\n",
            "=========> Train acc: 0.071 | Test acc: 0.044\n",
            "Epoch  27 | Train Loss: 2329.7444076538086\n",
            "=========> Train acc: 0.072 | Test acc: 0.045\n",
            "Epoch  28 | Train Loss: 2313.8951439857483\n",
            "=========> Train acc: 0.074 | Test acc: 0.042\n",
            "Epoch  29 | Train Loss: 2299.4343724250793\n",
            "=========> Train acc: 0.073 | Test acc: 0.042\n",
            "Epoch  30 | Train Loss: 2282.5908455848694\n",
            "=========> Train acc: 0.075 | Test acc: 0.041\n",
            "Epoch  31 | Train Loss: 2269.5894446372986\n",
            "=========> Train acc: 0.077 | Test acc: 0.042\n",
            "Epoch  32 | Train Loss: 2252.0222783088684\n",
            "=========> Train acc: 0.076 | Test acc: 0.040\n",
            "Epoch  33 | Train Loss: 2232.8331503868103\n",
            "=========> Train acc: 0.080 | Test acc: 0.042\n",
            "Epoch  34 | Train Loss: 2220.2238249778748\n",
            "=========> Train acc: 0.083 | Test acc: 0.043\n",
            "Epoch  35 | Train Loss: 2203.2780833244324\n",
            "=========> Train acc: 0.083 | Test acc: 0.041\n",
            "Epoch  36 | Train Loss: 2183.51087808609\n",
            "=========> Train acc: 0.082 | Test acc: 0.040\n",
            "Epoch  37 | Train Loss: 2170.1410913467407\n",
            "=========> Train acc: 0.084 | Test acc: 0.040\n",
            "Epoch  38 | Train Loss: 2152.742006778717\n",
            "=========> Train acc: 0.087 | Test acc: 0.039\n",
            "Epoch  39 | Train Loss: 2135.7672929763794\n",
            "=========> Train acc: 0.086 | Test acc: 0.043\n",
            "Epoch  40 | Train Loss: 2120.645248413086\n",
            "=========> Train acc: 0.090 | Test acc: 0.040\n",
            "Epoch  41 | Train Loss: 2112.3829946517944\n",
            "=========> Train acc: 0.087 | Test acc: 0.040\n",
            "Epoch  42 | Train Loss: 2098.974404811859\n",
            "=========> Train acc: 0.093 | Test acc: 0.038\n",
            "Epoch  43 | Train Loss: 2085.33935213089\n",
            "=========> Train acc: 0.095 | Test acc: 0.038\n",
            "Epoch  44 | Train Loss: 2065.955726623535\n",
            "=========> Train acc: 0.097 | Test acc: 0.038\n",
            "Epoch  45 | Train Loss: 2057.635359764099\n",
            "=========> Train acc: 0.102 | Test acc: 0.038\n",
            "Epoch  46 | Train Loss: 2041.6935992240906\n",
            "=========> Train acc: 0.100 | Test acc: 0.040\n",
            "Epoch  47 | Train Loss: 2034.548701763153\n",
            "=========> Train acc: 0.101 | Test acc: 0.039\n",
            "Epoch  48 | Train Loss: 2017.0547070503235\n",
            "=========> Train acc: 0.104 | Test acc: 0.037\n",
            "Epoch  49 | Train Loss: 2010.3251872062683\n",
            "=========> Train acc: 0.107 | Test acc: 0.036\n"
          ]
        }
      ],
      "source": [
        "\n",
        "n_epochs = 50\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  total_loss = 0.\n",
        "  model.train()\n",
        "  for data in train_loader:\n",
        "    model.zero_grad()\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "    # ìˆ˜ë§ì€ ê°’ì„ ì˜ˆì¸¡í•´ì„œ ê°€ì¥ í° ê°’ì„ ê³ ë¥´ëŠ”ê²Œ í¬ì¸íŠ¸\n",
        "    preds = model(inputs)\n",
        "    loss = loss_fn(preds, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    train_acc = accuracy(model, train_loader)\n",
        "    test_acc = accuracy(model, test_loader)\n",
        "    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqZays2yb8Ja"
      },
      "source": [
        "ì •í™•ë„ê°€ 0.1 ëŒ€ì¸ë°.. ì œëŒ€ë¡œ í•™ìŠµëœ ê±° ë§ë‚˜ìš”ğŸ¥º"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import softmax\n",
        "\n",
        "# 1. í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ 1ê°œ ê°€ì ¸ì˜¤ê¸°\n",
        "sample = test_ds[18]['text']\n",
        "tokens = tokenizer(sample, truncation=True, max_length=400).input_ids\n",
        "\n",
        "# 2. ì •ë‹µì€ ë’¤ì—ì„œ -3 ì¸ë±ìŠ¤ì¸ í† í°, ì…ë ¥ì€ ê·¸ ì•ë¶€ë¶„\n",
        "target_ids = tokens[-3]         # ì •ë‹µ\n",
        "input_ids = tokens[:-3]          # ì…ë ¥\n",
        "\n",
        "# 3. í…ì„œë¡œ ë³€í™˜í•´ì„œ ë°°ì¹˜ì²˜ëŸ¼ ë§Œë“¤ê¸°\n",
        "input_tensor = torch.LongTensor([input_ids]).to(device)  # shape: (1, L)\n",
        "\n",
        "# 4. ëª¨ë¸ì— ë„£ê¸°\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(input_tensor)\n",
        "    pred_ids = logits.argmax(dim=-1)[0]\n",
        "\n",
        "# 5. ì˜ˆì¸¡ ê²°ê³¼ ë””ì½”ë”©\n",
        "pred_tokens = tokenizer.decode(pred_ids.tolist()) #í† í° ID -> ë¬¸ì\n",
        "target_tokens = tokenizer.decode(target_ids)\n",
        "\n",
        "print(\"ğŸŸ¦ ì „ì²´ ì…ë ¥ ë¬¸ì¥:\", sample)\n",
        "print(\"ğŸŸ¦ ì…ë ¥ ë¬¸ì¥:\", tokenizer.decode(input_ids))\n",
        "print(\"ğŸŸ© ì •ë‹µ í† í°:\", target_tokens)\n",
        "print(\"ğŸŸ¥ ì˜ˆì¸¡ í† í°:\", pred_tokens)\n"
      ],
      "metadata": {
        "id": "u-I121osywgW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83aeeaff-3008-4213-d1d3-d07ba4ed6b12"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŸ¦ ì „ì²´ ì…ë ¥ ë¬¸ì¥: Ben, (Rupert Grint), is a deeply unhappy adolescent, the son of his unhappily married parents. His father, (Nicholas Farrell), is a vicar and his mother, (Laura Linney), is ... well, let's just say she's a somewhat hypocritical soldier in Jesus' army. It's only when he takes a summer job as an assistant to a foul-mouthed, eccentric, once-famous and now-forgotten actress Evie Walton, (Julie Walters), that he finally finds himself in true 'Harold and Maude' fashion. Of course, Evie is deeply unhappy herself and it's only when these two sad sacks find each other that they can put their mutual misery aside and hit the road to happiness.<br /><br />Of course it's corny and sentimental and very predictable but it has a hard side to it, too and Walters, who could sleep-walk her way through this sort of thing if she wanted, is excellent. It's when she puts the craziness to one side and finds the pathos in the character, (like hitting the bottle and throwing up in the sink), that she's at her best. The problem is she's the only interesting character in the film (and it's not because of the script which doesn't do anybody any favours). Grint, on the other hand, isn't just unhappy; he's a bit of a bore as well while Linney's starched bitch is completely one-dimensional. (Still, she's got the English accent off pat). The best that can be said for it is that it's mildly enjoyable - with the emphasis on the mildly.\n",
            "ğŸŸ¦ ì…ë ¥ ë¬¸ì¥: [CLS] ben, ( rupert grint ), is a deeply unhappy adolescent, the son of his unhappily married parents. his father, ( nicholas farrell ), is a vicar and his mother, ( laura linney ), is... well, let ' s just say she ' s a somewhat hypocritical soldier in jesus ' army. it ' s only when he takes a summer job as an assistant to a foul - mouthed, eccentric, once - famous and now - forgotten actress evie walton, ( julie walters ), that he finally finds himself in true ' harold and maude ' fashion. of course, evie is deeply unhappy herself and it ' s only when these two sad sacks find each other that they can put their mutual misery aside and hit the road to happiness. < br / > < br / > of course it ' s corny and sentimental and very predictable but it has a hard side to it, too and walters, who could sleep - walk her way through this sort of thing if she wanted, is excellent. it ' s when she puts the craziness to one side and finds the pathos in the character, ( like hitting the bottle and throwing up in the sink ), that she ' s at her best. the problem is she ' s the only interesting character in the film ( and it ' s not because of the script which doesn ' t do anybody any favours ). grint, on the other hand, isn ' t just unhappy ; he ' s a bit of a bore as well while linney ' s starched bitch is completely one - dimensional. ( still, she ' s got the english accent off pat ). the best that can be said for it is that it ' s mildly enjoyable - with the emphasis on the\n",
            "ğŸŸ© ì •ë‹µ í† í°: mildly\n",
            "ğŸŸ¥ ì˜ˆì¸¡ í† í°: .\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "name": "Transformer_ipynbá„‹á…´_á„‰á…¡á„‡á…©á†«.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}