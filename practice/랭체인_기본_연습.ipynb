{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. LangChain 설치"
      ],
      "metadata": {
        "id": "PIwQI2NVAxUE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoqSUsj6AgWt",
        "outputId": "22e7d7f6-4241-4c23-9e9d-c6b199f58ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.14-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.52)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting langchain-core<1.0.0,>=0.3.51 (from langchain)\n",
            "  Downloading langchain_core-0.3.55-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
            "Downloading langchain_openai-0.3.14-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading langchain_core-0.3.55-py3-none-any.whl (434 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, tiktoken, dotenv, langchain-core, langchain-openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.52\n",
            "    Uninstalling langchain-core-0.3.52:\n",
            "      Successfully uninstalled langchain-core-0.3.52\n",
            "Successfully installed dotenv-0.9.9 langchain-core-0.3.55 langchain-openai-0.3.14 python-dotenv-1.1.0 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-openai openai dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.openAI API 활용한 기본 예제"
      ],
      "metadata": {
        "id": "Xg0b66qxAv8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "import getpass\n",
        "\n",
        "\n",
        "openai_api_key = getpass.getpass(\"OpenAI API Key: \")\n",
        "\n",
        "# openAI 모델 불러오기\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\", api_key=openai_api_key)\n",
        "\n",
        "# 질문하기\n",
        "res = llm.invoke(\"LangChain이 뭐야?\")\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJNCI44SA5Cz",
        "outputId": "fbefda65-8c2b-42c7-f6f4-a19837044e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API Key: ··········\n",
            "LangChain은 대화형 인공지능 애플리케이션을 쉽고 효과적으로 개발할 수 있도록 도와주는 오픈소스 프레임워크입니다. 주로 자연어 처리(NLP)와 대규모 언어 모델(LLM, Large Language Models)을 활용하는 데 중점을 두고 있으며, 여러 컴포넌트(예: 프롬프트 관리, 체인 구성, 메모리 유지, 외부 데이터 소스 연동 등)를 결합하여 복잡한 언어 기반 워크플로우를 구축할 수 있게 해줍니다.\n",
            "\n",
            "주요 특징은 다음과 같습니다:\n",
            "\n",
            "- **모듈화된 체인 구성**: 여러 언어 모델 호출이나 데이터 처리 작업을 단계별로 연결해 복잡한 로직을 구현 가능\n",
            "- **프롬프트 템플릿 관리**: 반복해서 사용할 수 있는 프롬프트를 체계적으로 관리 및 재사용\n",
            "- **메모리 기능**: 이전 대화나 상태를 유지해 대화형 애플리케이션에 적합\n",
            "- **데이터 연동**: 데이터베이스, 문서, API 등 외부 소스와 통합해 LLM이 더 풍부한 데이터를 활용할 수 있도록 지원\n",
            "- **다양한 언어 모델 지원**: OpenAI, Cohere, Hugging Face 등 여러 LLM 서비스를 쉽게 연동\n",
            "\n",
            "결론적으로 LangChain은 복잡한 자연어 처리 파이프라인과 대화형 AI 솔루션을 빠르게 만들고 관리할 수 있게 해 주는 도구입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 프롬프트 템플릿"
      ],
      "metadata": {
        "id": "EVeSUjWWBmhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "# RAG에서 사용할 Langchain 프롬프트 불러오기\n",
        "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# 예제 프롬프트 출력\n",
        "print(rag_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K9ojcXtBq_T",
        "outputId": "efa22433-6018-4b47-a792-e89931639c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:280: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 체이닝 사용하기"
      ],
      "metadata": {
        "id": "aUkLLoxHFFcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "# 체이닝 - 프롬프트와 모델을 연결\n",
        "pipeline = rag_prompt | llm\n",
        "\n",
        "# 실행\n",
        "response = pipeline.invoke({\n",
        "    \"context\" : \"LangChain은 AI 개발에 유용한 도구입니다.\",\n",
        "    \"question\": \"Langchain의 장점은?\"\n",
        "})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq5ddVBTFISE",
        "outputId": "0e05de50-0275-49a6-f014-5612499494fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain의 장점은 AI 개발에 특화된 도구로, 효율적이고 체계적인 AI 애플리케이션 개발을 지원한다는 점입니다. 이를 통해 복잡한 작업을 쉽게 처리할 수 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실습 1 : 다양한 프롬프트 템플릿 사용해보기"
      ],
      "metadata": {
        "id": "56su4eyRFgP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"당신은 친절한 도우미입니다. 질문에 성실히 답변하세요. {question}\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "formatted_prompt = prompt.format(question=\"오늘의 날씨는?\")\n",
        "print(formatted_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niQo4IQnFmQf",
        "outputId": "9bb758d1-c3e9-4114-bd2f-69e1d9d722de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "당신은 친절한 도우미입니다. 질문에 성실히 답변하세요. 오늘의 날씨는?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 다중 변수 템플릿\n",
        "multi_template = (\n",
        "    \"질문에 아래 정보를 반영해서 답해주세요. \\n\"\n",
        "    \"배경정보 : {context}\\n\"\n",
        "    \"질문 : {question}\"\n",
        ")\n",
        "multi_prompt = PromptTemplate.from_template(multi_template)\n",
        "\n",
        "print(multi_prompt.format(context=\"LangChain은 AI 개발에 유용한 도구입니다.\", question=\"Langchain의 장점은?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnGv7MT5F6Ks",
        "outputId": "7d6d3aba-4b62-46a2-861f-dc97b75530d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문에 아래 정보를 반영해서 답해주세요. \n",
            "배경정보 : LangChain은 AI 개발에 유용한 도구입니다.\n",
            "질문 : Langchain의 장점은?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 다양한 OpenAI 모델 불러보기"
      ],
      "metadata": {
        "id": "lEs1K3zjGPZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_gpt_nano = ChatOpenAI(model=\"gpt-4.1-nano\",api_key=openai_api_key)\n",
        "response_nano = llm_gpt_nano.invoke(\"Langchain이 뭐야?\")\n",
        "print(\"gpt-4.1-nano 응답: \", response_nano)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG-Yp6qzGVn_",
        "outputId": "29af285e-6f28-4a0c-c3da-fc79d468caf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-4.1-nano 응답:  content='LangChain은 자연어 처리 및 인공지능 애플리케이션 개발에 도움이 되는 오픈소스 프레임워크입니다. 주로 대규모 언어 모델(LLMs)을 활용하여 다양한 작업을 쉽게 구축하고 관리할 수 있도록 설계되었습니다.  \\n\\nLangChain의 주요 특징은 다음과 같습니다:  \\n1. **체인(Chains) 구성**: 여러 개의 작업이나 단계를 연결하여 복잡한 프로세스를 쉽게 만들 수 있습니다.  \\n2. **프롬프트 템플레이트**: 사용자 정의 프롬프트를 쉽게 설계하고 재사용할 수 있도록 도와줍니다.  \\n3. **모듈화된 컴포넌트**: 문서 이해, 질의응답, 대화형 인터페이스 등 다양한 기능을 모듈로 제공하여 유연하게 활용 가능.  \\n4. **통합된 데이터 소스**: 다양한 데이터 소스(문서, 데이터베이스, API 등)와 쉽게 연결할 수 있어, 실시간 정보 활용이 용이합니다.  \\n\\nLangChain은 Python을 기반으로 하며, 개발자가 대화형 AI, 자동화 도구, 정보 검색 시스템, 챗봇 등 다양한 어플리케이션을 빠르게 개발할 수 있도록 지원합니다.  \\n\\n간단히 말해, LangChain은 언어 모델을 활용하는 애플리케이션 개발을 매우 편리하게 만들어주는 도구라고 볼 수 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 312, 'prompt_tokens': 13, 'total_tokens': 325, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_8fd43718b3', 'id': 'chatcmpl-BP9mRdI9a07C8Kdxdxu8otNNj6QKQ', 'finish_reason': 'stop', 'logprobs': None} id='run-27fcfc0f-071b-4da3-9ad8-6255f09a9e00-0' usage_metadata={'input_tokens': 13, 'output_tokens': 312, 'total_tokens': 325, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory로 대화 상태 유지하기"
      ],
      "metadata": {
        "id": "gbclcmkXGlat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import VERBOSE\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "print(conversation.invoke(\"안녕하세요\")['response'])\n",
        "print(conversation.invoke(\"내가 방금 뭐라고 했어?\")['response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciYGrqvZIWqt",
        "outputId": "08ec889a-4e84-4a9e-da1c-0d20ea1b5b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[]\n",
            "Human: 안녕하세요\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "안녕하세요! 만나서 반갑습니다. 오늘 기분은 어떠세요? 도움이 필요한 것이 있으면 언제든지 말씀해 주세요!\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='안녕하세요', additional_kwargs={}, response_metadata={}), AIMessage(content='안녕하세요! 만나서 반갑습니다. 오늘 기분은 어떠세요? 도움이 필요한 것이 있으면 언제든지 말씀해 주세요!', additional_kwargs={}, response_metadata={})]\n",
            "Human: 내가 방금 뭐라고 했어?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "방금 \"안녕하세요\"라고 하셨어요. 반갑게 인사해 주셔서 감사합니다! 또 궁금한 것이 있으면 언제든지 물어보세요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 체이닝 확장 실습(파서 추가 등)"
      ],
      "metadata": {
        "id": "OcNvrsxUI2zM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "#프롬프트 -> LLM -> 파싱\n",
        "pipeline = rag_prompt | llm | StrOutputParser()\n",
        "\n",
        "output = pipeline.invoke({\"context\": \"Langchain은 프레임워크입니다.\", \"question\": \"Langchain이 뭔가요?\"})\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X1hnqFuJWCE",
        "outputId": "7f163720-03b5-40d3-d615-bd096d98852d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Langchain은 언어 모델을 활용한 애플리케이션을 개발하기 위한 프레임워크입니다. 다양한 도구와 기능을 제공하여 효율적인 작업이 가능하게 합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RunnableLambda로 전처리 후가하기"
      ],
      "metadata": {
        "id": "X_fV0S8DJteD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# 사용자 입력을 전처리하는 람다\n",
        "preprocess = RunnableLambda(lambda x:{\"context\": x[\"context\"].upper(), \"question\": x['question']})\n",
        "\n",
        "# 전처리 -> 프롬프트 -> 모델 -> 파서\n",
        "pipeline = preprocess | rag_prompt | llm | StrOutputParser()\n",
        "\n",
        "output = pipeline.invoke({\"context\": \"LangChain은 유용한 도구입니다.\", \"question\": \"왜 좋은가요?\"})\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CVbiAzHJw9A",
        "outputId": "2686fb96-6b3c-4fe0-e4e5-718f58f1fc00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LANGCHAIN은 다양한 언어 모델과 쉽게 연동되어 복잡한 작업을 간단하게 처리할 수 있어 좋습니다. 또한, 모듈화된 구조로 개발과 확장이 용이합니다. 이로 인해 생산성과 효율성이 크게 향상됩니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM 응답 후 요약 / 필터링 처리"
      ],
      "metadata": {
        "id": "fpW2t1xaKMMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 후처리용 람다 추가\n",
        "postprocess = RunnableLambda(lambda x: x[:30] + \"...(요약됨)\" if isinstance(x, str) else x)\n",
        "\n",
        "\n",
        "# 프롬프트 → LLM → 파싱 → 후처리\n",
        "full_chain = rag_prompt | llm | StrOutputParser() | postprocess\n",
        "\n",
        "result = full_chain.invoke({\"context\": \"LangChain은 다양한 기능을 가진 프레임워크입니다. 텍스트 기반 처리, 체이닝, 프롬프트 관리 등을 지원합니다.\",\n",
        "                            \"question\": \"LangChain의 기능을 요약해줘\"})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duxnkBfKKRpF",
        "outputId": "0055279f-ee3f-47e6-f677-a80055ef2161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain은 텍스트 기반 처리, 체이닝, 프롬프...(요약됨)\n"
          ]
        }
      ]
    }
  ]
}